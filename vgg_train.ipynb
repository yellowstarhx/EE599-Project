{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg_train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yellowstarhx/EE599-Project/blob/master/vgg_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VChhOiPuMokc",
        "colab_type": "code",
        "outputId": "0a049826-bfb5-4274-b95d-66c976f1c3fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "# Reference: https://blog.csdn.net/rocachilles/article/details/87894808\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from skimage import io, transform\n",
        "import glob\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
        "# path = '/content/drive/My Drive/train_vgg/'          # for one dataset cross validation\n",
        "train_path = '/content/drive/My Drive/train_vgg/' # for train and test set\n",
        "test_path = '/content/drive/My Drive/test_vgg/'\n",
        "w = 224\n",
        "h = 224\n",
        "c = 3\n",
        "n_class = 40\n",
        "\n",
        "def read_img(path):\n",
        "    cate   = [path + x for x in os.listdir(path) if os.path.isdir(path + x)]\n",
        "    imgs   = []\n",
        "    labels = []\n",
        "    label_list = np.eye(n_class)\n",
        "    for idx, folder in enumerate(cate):            #search folder\n",
        "        for im in glob.glob(folder + '/*.jpg'):    #change doc type if necessary\n",
        "            img = io.imread(im)\n",
        "            img = transform.resize(img, (w, h, c))\n",
        "            imgs.append(img)                        # (sum,224,224,3)\n",
        "            labels.append(label_list[idx])          # (sum,4)         \n",
        "        for im in glob.glob(folder + '/*.png'):    #change doc type if necessary\n",
        "            img = io.imread(im)\n",
        "            img = transform.resize(img, (w, h, c))\n",
        "            imgs.append(img)                        # (sum,224,224,3)\n",
        "            labels.append(label_list[idx])          # (sum,4)                         \n",
        "    return np.asarray(imgs, np.float32), np.asarray(labels, np.float32)\n",
        "\n",
        "#------------------train and test set------------\n",
        "data, label = read_img(train_path)\n",
        "   \n",
        "num_example = data.shape[0]                        \n",
        "arr = np.arange(num_example)                    \n",
        "np.random.shuffle(arr)                           # random sequence\n",
        "x_train = data[arr]\n",
        "y_train = label[arr]\n",
        "\n",
        "s = num_example\n",
        "\n",
        "data_t, label_t = read_img(test_path)\n",
        "   \n",
        "s_test = data_t.shape[0]                        \n",
        "arr = np.arange(s_test)                    \n",
        "np.random.shuffle(arr)                           # random sequence\n",
        "x_val = data_t[arr]\n",
        "y_val = label_t[arr]\n",
        "\n",
        "# ------------------one dataset cross validation ----------\n",
        "# data, label = read_img(path)\n",
        "   \n",
        "# num_example = data.shape[0]                        \n",
        "# arr = np.arange(num_example)                    \n",
        "# np.random.shuffle(arr)                           # random sequence\n",
        "# data = data[arr]\n",
        "# label = label[arr]\n",
        "\n",
        "# ratio = 0.8\n",
        "# s = np.int(num_example * ratio)\n",
        "# x_train = data[:s]                         # (sum_train,224,224,3)\n",
        "# y_train = label[:s]                        # (sum_train,4)\n",
        "# x_val   = data[s:]\n",
        "# y_val   = label[s:]    \n",
        "\n",
        "#------------------vgg16 structure----------------\n",
        " \n",
        "x = tf.placeholder(tf.float32, shape=[None, h, w, c])\n",
        "y = tf.placeholder(tf.float32, shape=[None, n_class])     \n",
        "\n",
        "#----------------- conv1 ------------------------\n",
        "\n",
        "w_conv1_1 = tf.Variable(tf.truncated_normal([3, 3, 3, 64], stddev=0.1))\n",
        "b_conv1_1 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
        "L_conv1_1 = tf.nn.relu(tf.nn.conv2d(x, w_conv1_1,strides=[1, 1, 1, 1], padding='SAME') + b_conv1_1)\n",
        "\n",
        "w_conv1_2 = tf.Variable(tf.truncated_normal([3, 3, 64, 64], stddev=0.1))\n",
        "b_conv1_2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
        "L_conv1_2 = tf.nn.relu(tf.nn.conv2d(L_conv1_1, w_conv1_2,strides=[1, 1, 1, 1], padding='SAME') + b_conv1_2)\n",
        "\n",
        "L_pool1 = tf.nn.max_pool(L_conv1_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "#----------------- conv2 ------------------------\n",
        "\n",
        "w_conv2_1 = tf.Variable(tf.truncated_normal([3, 3, 64, 128], stddev=0.1))\n",
        "b_conv2_1 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
        "L_conv2_1 = tf.nn.relu(tf.nn.conv2d(L_pool1, w_conv2_1,strides=[1, 1, 1, 1], padding='SAME') + b_conv2_1)\n",
        "\n",
        "w_conv2_2 = tf.Variable(tf.truncated_normal([3, 3, 128, 128], stddev=0.1))\n",
        "b_conv2_2 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
        "L_conv2_2 = tf.nn.relu(tf.nn.conv2d(L_conv2_1, w_conv2_2,strides=[1, 1, 1, 1], padding='SAME') + b_conv2_2)\n",
        "\n",
        "L_pool2 = tf.nn.max_pool(L_conv2_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "#-------------------conv3 -------------------------\n",
        "\n",
        "w_conv3_1 = tf.Variable(tf.truncated_normal([3, 3, 128, 256], stddev=0.1))\n",
        "b_conv3_1 = tf.Variable(tf.constant(0.1, shape=[256]))\n",
        "L_conv3_1 = tf.nn.relu(tf.nn.conv2d(L_pool2, w_conv3_1,strides=[1, 1, 1, 1], padding='SAME') + b_conv3_1)\n",
        "\n",
        "\n",
        "w_conv3_2 = tf.Variable(tf.truncated_normal([3, 3, 256, 256], stddev=0.1))\n",
        "b_conv3_2 = tf.Variable(tf.constant(0.1, shape=[256]))\n",
        "L_conv3_2 = tf.nn.relu(tf.nn.conv2d(L_conv3_1, w_conv3_2,strides=[1, 1, 1, 1], padding='SAME') + b_conv3_2)\n",
        "\n",
        "\n",
        "w_conv3_3 = tf.Variable(tf.truncated_normal([3, 3, 256, 256], stddev=0.1))\n",
        "b_conv3_3 = tf.Variable(tf.constant(0.1, shape=[256]))\n",
        "L_conv3_3 = tf.nn.relu(tf.nn.conv2d(L_conv3_2, w_conv3_3,strides=[1, 1, 1, 1], padding='SAME') + b_conv3_3)\n",
        "\n",
        "L_pool3 = tf.nn.max_pool(L_conv3_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "#---------------------- conv4 -----------------------------------\n",
        "\n",
        "w_conv4_1 = tf.Variable(tf.truncated_normal([3, 3, 256, 512], stddev=0.1))\n",
        "b_conv4_1 = tf.Variable(tf.constant(0.1, shape=[512]))\n",
        "L_conv4_1 = tf.nn.relu(tf.nn.conv2d(L_pool3, w_conv4_1,strides=[1, 1, 1, 1], padding='SAME') + b_conv4_1)\n",
        "\n",
        "\n",
        "w_conv4_2 = tf.Variable(tf.truncated_normal([3, 3, 512, 512], stddev=0.1))\n",
        "b_conv4_2 = tf.Variable(tf.constant(0.1, shape=[512]))\n",
        "L_conv4_2 = tf.nn.relu(tf.nn.conv2d(L_conv4_1, w_conv4_2,strides=[1, 1, 1, 1], padding='SAME') + b_conv4_2)\n",
        "\n",
        "\n",
        "w_conv4_3 = tf.Variable(tf.truncated_normal([3, 3, 512, 512], stddev=0.1))\n",
        "b_conv4_3 = tf.Variable(tf.constant(0.1, shape=[512]))\n",
        "L_conv4_3 = tf.nn.relu(tf.nn.conv2d(L_conv4_2, w_conv4_3,strides=[1, 1, 1, 1], padding='SAME') + b_conv4_3)\n",
        "\n",
        "L_pool4 = tf.nn.max_pool(L_conv4_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "#---------------------- conv5 -----------------------------------\n",
        "\n",
        "w_conv5_1 = tf.Variable(tf.truncated_normal([3, 3, 512, 512], stddev=0.1))\n",
        "b_conv5_1 = tf.Variable(tf.constant(0.1, shape=[512]))\n",
        "L_conv5_1 = tf.nn.relu(tf.nn.conv2d(L_pool4, w_conv5_1,strides=[1, 1, 1, 1], padding='SAME') + b_conv5_1)\n",
        "\n",
        "\n",
        "w_conv5_2 = tf.Variable(tf.truncated_normal([3, 3, 512, 512], stddev=0.1))\n",
        "b_conv5_2 = tf.Variable(tf.constant(0.1, shape=[512]))\n",
        "L_conv5_2 = tf.nn.relu(tf.nn.conv2d(L_conv5_1, w_conv5_2,strides=[1, 1, 1, 1], padding='SAME') + b_conv5_2)\n",
        "\n",
        "\n",
        "w_conv5_3 = tf.Variable(tf.truncated_normal([3, 3, 512, 512], stddev=0.1))\n",
        "b_conv5_3 = tf.Variable(tf.constant(0.1, shape=[512]))\n",
        "L_conv5_3 = tf.nn.relu(tf.nn.conv2d(L_conv5_2, w_conv5_3,strides=[1, 1, 1, 1], padding='SAME') + b_conv5_3)\n",
        "\n",
        "L_pool5 = tf.nn.max_pool(L_conv5_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "#------------------------ fully connected 6 -------------------------------------\n",
        "shape6 = int(np.prod(L_pool5.get_shape()[1:]))   \n",
        "w_fc6 = tf.Variable(tf.truncated_normal([shape6, 4096], stddev=0.1))\n",
        "b_fc6 = tf.Variable(tf.constant(0.1, shape=[4096]))\n",
        "f_fc6 = tf.reshape(L_pool5, [-1, shape6])\n",
        "L_fc6 = tf.nn.relu(tf.matmul(f_fc6, w_fc6) + b_fc6)\n",
        "\n",
        "#------------------------  fully connected 7  ----------------------------\n",
        "\n",
        "# ---------------------- Drop --------------------------\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "d_fc7 = tf.nn.dropout(L_fc6, keep_prob)\n",
        "\n",
        "w_fc7 = tf.Variable(tf.truncated_normal([4096, 4096], stddev=0.1))\n",
        "b_fc7 = tf.Variable(tf.constant(0.1, shape=[4096]))\n",
        "L_fc7 = tf.nn.relu(tf.matmul(d_fc7, w_fc7) + b_fc7)\n",
        "\n",
        "#------------------------ fully connected 8 ------------------------------------\n",
        "\n",
        "w_fc8 = tf.Variable(tf.truncated_normal([4096, n_class], stddev=0.1))\n",
        "b_fc8 = tf.Variable(tf.constant(0.1, shape=[n_class]))\n",
        "L_fc8 = tf.matmul(L_fc7, w_fc8) + b_fc8\n",
        "\n",
        "#-------------------- final output -------------------------------\n",
        "\n",
        "# y_conv = tf.nn.softmax(L_fc8)\n",
        "y_conv = L_fc8\n",
        "y_ = tf.nn.softmax(L_fc8)\n",
        "\n",
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_conv))\n",
        "train_step = tf.train.AdamOptimizer((1e-4)).minimize(cross_entropy)\n",
        "prediction_cls = tf.argmax(y_conv,1) #the actual class\n",
        "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# --------------- save the model ----------------------------\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "#------------------ run --------------------------------\n",
        "\n",
        "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    print (\"Input %s images, %s labels\" % (s, s)) #training set\n",
        "\n",
        "    # divide batches\n",
        "    batch_size = 2\n",
        "    epochs = 50\n",
        "    batches_count = int(s / batch_size) # iterations per epoch\n",
        "    remainder = s % batch_size\n",
        "    print (\"Dataset is divided into %s batches,  %s images per batch, %s images for last batch\" % (batches_count+1, batch_size, remainder))\n",
        "    prev_loss = 0     # determination of convergence \n",
        "    stable_epoch = 0  # determination of convergence \n",
        "\n",
        "    # training\n",
        "    for ep in range(epochs):\n",
        "        ep_acc = 0\n",
        "        ep_loss = 0\t\n",
        "#         ep_preclass = []\n",
        "        # transfer input to np.array\n",
        "        for n in range(batches_count):          \n",
        "            train_step.run(feed_dict={x: x_train[n*batch_size:(n+1)*batch_size], y: y_train[n*batch_size:(n+1)*batch_size], keep_prob: 0.99})\n",
        "            acc, loss, pre = sess.run([accuracy, cross_entropy, prediction_cls], feed_dict={x: x_train[n*batch_size:(n+1)*batch_size], y: y_train[n*batch_size:(n+1)*batch_size], keep_prob: 0.99})\n",
        "            ep_acc = ep_acc + acc\n",
        "            ep_loss = ep_loss + loss\n",
        "#             ep_preclass = ep_preclass.extend(pre)\n",
        "        ep_acc = ep_acc * batch_size\n",
        "        ep_loss = ep_loss * batch_size\t\t\n",
        "        if remainder > 0:\n",
        "            start_index = batches_count * batch_size;\n",
        "            train_step.run(feed_dict={x: x_train[start_index:s-1], y: y_train[start_index:s-1], keep_prob: 0.99})\n",
        "            acc, loss, pre = sess.run([accuracy, cross_entropy, prediction_cls], feed_dict={x: x_train[start_index:s-1], y: y_train[start_index:s-1], keep_prob: 0.99})\n",
        "            acc = acc * remainder\n",
        "            loss = loss * remainder\n",
        "            ep_acc = ep_acc + acc\n",
        "            ep_loss = ep_loss + loss\n",
        "#             ep_preclass = ep_preclass.extend(pre)\n",
        "        ep_acc = ep_acc / s\n",
        "        ep_loss = ep_loss / s        \t\t\t\n",
        "        if prev_loss == ep_loss:   # determination of convergence\n",
        "            stable_epoch = stable_epoch + 1\n",
        "        else:\n",
        "            stable_epoch = 0\n",
        "            prev_loss = ep_loss\n",
        "        if ep%5 == 0:\n",
        "            print ('epoch %d: training accuracy %s' % (ep, ep_acc))\n",
        "#             print(pre)\n",
        "#             print (ep_preclass)\n",
        "            print ('epoch %d: loss %s' % (ep, ep_loss))\n",
        "            val_acc, val_loss, val_pre = sess.run([accuracy, cross_entropy, prediction_cls],feed_dict={x: x_val, y: y_val, keep_prob: 1.0})\n",
        "            print ('epoch %d: testing accuracy %s' % (ep, val_acc))\n",
        "#             print (val_pre)\n",
        "        if stable_epoch > 20:       #converges after 20 epoches\n",
        "            break\n",
        "    save_path = saver.save(sess,\"/content/drive/My Drive/model/vgg.ckpt\")  \n",
        "    print ('--------- training finished! --------------')\n",
        "    print ('Total epochs %d: training accuracy %s' % (ep, ep_acc))\n",
        "#     print (pre)\n",
        "#     print (ep_preclass)\n",
        "    print ('Total epochs %d: loss %s' % (ep, ep_loss))\n",
        "    val_acc, val_loss, val_pre = sess.run([accuracy, cross_entropy, prediction_cls],feed_dict={x: x_val, y: y_val, keep_prob: 1.0})\n",
        "    print ('Total epochs %d: testing accuracy %s' % (ep, val_acc))\n",
        "#     print (val_pre)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
            "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Input 200 images, 200 labels\n",
            "Dataset is divided into 101 batches,  2 images per batch, 0 images for last batch\n",
            "epoch 0: training accuracy 0.165\n",
            "epoch 0: loss 10939496.0478125\n",
            "epoch 0: testing accuracy 0.025\n",
            "epoch 5: training accuracy 0.305\n",
            "epoch 5: loss 253283.273984375\n",
            "epoch 5: testing accuracy 0.1\n",
            "epoch 10: training accuracy 0.5\n",
            "epoch 10: loss 96042.765625\n",
            "epoch 10: testing accuracy 0.175\n",
            "epoch 15: training accuracy 0.55\n",
            "epoch 15: loss 47417.5525390625\n",
            "epoch 15: testing accuracy 0.125\n",
            "epoch 20: training accuracy 0.67\n",
            "epoch 20: loss 24192.379453125\n",
            "epoch 20: testing accuracy 0.15\n",
            "epoch 25: training accuracy 0.9\n",
            "epoch 25: loss 4452.4116796875\n",
            "epoch 25: testing accuracy 0.175\n",
            "epoch 30: training accuracy 0.82\n",
            "epoch 30: loss 12773.5664453125\n",
            "epoch 30: testing accuracy 0.225\n",
            "epoch 35: training accuracy 0.93\n",
            "epoch 35: loss 1693.424375\n",
            "epoch 35: testing accuracy 0.225\n",
            "epoch 40: training accuracy 0.935\n",
            "epoch 40: loss 1261.880390625\n",
            "epoch 40: testing accuracy 0.325\n",
            "epoch 45: training accuracy 0.99\n",
            "epoch 45: loss 156.979375\n",
            "epoch 45: testing accuracy 0.4\n",
            "--------- training finished! --------------\n",
            "Total epochs 49: training accuracy 0.955\n",
            "Total epochs 49: loss 1277.78046875\n",
            "Total epochs 49: testing accuracy 0.525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZScWmZ6BPoTH",
        "colab_type": "code",
        "outputId": "e47ffe9a-5cb5-4d55-9b32-4ced9839e6d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}